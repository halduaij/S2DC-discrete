{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZasOJSvPPuG5JGKEqU20i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halduaij/S2DC-discrete/blob/main/s2dc_continuous.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1tY6HDbBQa8"
      },
      "outputs": [],
      "source": [
        "# First, install all required packages\n",
        "!apt-get update -qq\n",
        "!apt-get install -y python3-opengl > /dev/null\n",
        "!apt-get install -y xvfb > /dev/null\n",
        "!pip install gymnasium[mujoco] --quiet\n",
        "!pip install pyvirtualdisplay --quiet\n",
        "!pip install pyglet --quiet  # Additional dependency for rendering\n",
        "\n",
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from IPython.display import Image, clear_output\n",
        "\n",
        "# Set up the virtual display first\n",
        "print(\"Setting up virtual display...\")\n",
        "from pyvirtualdisplay import Display\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "\n",
        "# Configure MuJoCo rendering\n",
        "os.environ['MUJOCO_GL'] = 'egl'\n",
        "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
        "\n",
        "# Import gymnasium after display setup\n",
        "print(\"Importing gymnasium...\")\n",
        "import gymnasium as gym\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import csv\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "class CustomLogger:\n",
        "    def __init__(self, log_dir=\"logs\"):\n",
        "        # Create timestamp for unique experiment folder\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.log_dir = Path(log_dir) / timestamp\n",
        "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Define fieldnames for each file\n",
        "        self.training_fields = [\n",
        "            'step', 'loss_q_rs', 'loss_q_d', 'loss_n1', 'loss_n2',\n",
        "            'kl_div', 'actor_loss', 'alpha_loss', 'alpha_val',\n",
        "            'bellman_err','bellman_err2', 'value_err', 'weight_mean', 'tau_used'\n",
        "        ]\n",
        "\n",
        "        self.eval_fields = ['step', 'return_rs', 'return_d']\n",
        "\n",
        "        # Initialize CSV files\n",
        "        self.files = {\n",
        "            'training': self._init_csv('training_metrics.csv', self.training_fields),\n",
        "            'eval': self._init_csv('eval_metrics.csv', self.eval_fields)\n",
        "        }\n",
        "\n",
        "        # Save references to fieldnames\n",
        "        self.fieldnames = {\n",
        "            'training': self.training_fields,\n",
        "            'eval': self.eval_fields\n",
        "        }\n",
        "\n",
        "    def _init_csv(self, filename, fieldnames):\n",
        "        file = self.log_dir / filename\n",
        "        with open(file, 'w', newline='') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "        return file\n",
        "\n",
        "    def log(self, metrics, step):\n",
        "        metrics['step'] = step\n",
        "        with open(self.files['training'], 'a', newline='') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=self.fieldnames['training'])\n",
        "            writer.writerow(metrics)\n",
        "\n",
        "    def log_eval(self, eval_metrics, step):\n",
        "        eval_metrics['step'] = step\n",
        "        with open(self.files['eval'], 'a', newline='') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=self.fieldnames['eval'])\n",
        "            writer.writerow(eval_metrics)\n",
        "\n",
        "    def save_config(self, config_dict):\n",
        "        self.config = config_dict\n",
        "        with open(self.log_dir / 'config.json', 'w') as f:\n",
        "            json.dump(config_dict, f, indent=4)\n",
        "\n",
        "    def finish(self):\n",
        "        print(f\"Logs saved to: {self.log_dir}\")\n"
      ],
      "metadata": {
        "id": "S5vJZOgPBUr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "import wandb\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 1. Config and Utilities\n",
        "# ----------------------------------------------------\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        # Core\n",
        "        self.gamma = 0.99\n",
        "        self.batch_size = 256\n",
        "        self.buffer_capacity = int(1e6)\n",
        "        self.init_alpha = 1 # Add initial alpha value\n",
        "        self.q_lr = 3e-4\n",
        "        # Slower policy learning\n",
        "        self.policy_lr = 1e-4\n",
        "        self.policy_eval_start = 0  # or any integer steps you want\n",
        "\n",
        "        # Keep alpha learning rate low\n",
        "        self.alpha_lr =3e-4\n",
        "        # Network (unchanged)\n",
        "        self.hidden_sizes = [256, 256]\n",
        "        self.initial_learning_rate = 3e-4\n",
        "        self.target_entropy = None  # set automatically: -act_dim\n",
        "        self.init_alpha_rs = 1  # Initial alpha for RS policy\n",
        "        self.init_alpha_d = 1   # Initial alpha for D policy\n",
        "        # Warm-up logic\n",
        "        self.warm_up_steps = 3000       # full uniform\n",
        "        self.ramp_up_steps = 50000    # ramp from step=5000 -> step=15000\n",
        "        self.temp = 1.0\n",
        "        self.min_q_weight = 0.005  # varies by dataset type\n",
        "        self.num_random = 10\n",
        "        self.with_lagrange = False  # Set to True to use Lagrange version\n",
        "        self.lagrange_thresh = 10.0\n",
        "        # Temperature for reshuffling\n",
        "        self.initial_tau = 5.0\n",
        "        self.min_tau = 0.05\n",
        "        self.tau_decay = 0.999  # Faster decay\n",
        "\n",
        "        self.max_tau = 10.0\n",
        "        self.kl_div_threshold = 3.0\n",
        "\n",
        "        # Conservative penalty weight\n",
        "        self.lambda_cql = 10\n",
        "\n",
        "        # Target network updates\n",
        "        self.target_update_freq = 1\n",
        "        self.soft_update_tau = 0.005\n",
        "\n",
        "        # Logging, steps\n",
        "        self.max_gradient_steps = 35000\n",
        "        self.eval_interval = 10000\n",
        "        self.save_interval = 20000\n",
        "    # Learning rate schedules\n",
        "\n",
        "        # GMM config\n",
        "        self.gmm_num_components = 20\n",
        "        self.gmm_max_iter = 500\n",
        "\n",
        "    def get_tau(self, step) -> float:\n",
        "        tau_raw = tf.maximum(\n",
        "            self.min_tau,\n",
        "            self.initial_tau * (self.tau_decay ** tf.cast(step // 1000, tf.float32))\n",
        "        )\n",
        "        return tau_raw\n",
        "\n",
        "def set_seeds(seed_value=42):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    tf.random.set_seed(seed_value)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2. Network Architectures (unchanged)\n",
        "# ----------------------------------------------------\n",
        "def create_double_q_networks(state_dim, action_dim, hidden_sizes):\n",
        "    \"\"\"\n",
        "    Creates two Q-networks with proper initialization and normalization.\n",
        "    Uses orthogonal initialization and batch normalization.\n",
        "    \"\"\"\n",
        "    s_in = keras.Input(shape=(state_dim,), dtype=tf.float32)\n",
        "    a_in = keras.Input(shape=(action_dim,), dtype=tf.float32)\n",
        "    concat_in = layers.Concatenate(axis=-1)([s_in, a_in])\n",
        "    x1 = concat_in  # Remove BatchNorm here\n",
        "    # Q1 Network\n",
        "    for size in hidden_sizes:\n",
        "        x1 = layers.Dense(\n",
        "            size,\n",
        "            kernel_initializer='glorot_uniform',  # Default Xavier/Glorot init\n",
        "            bias_initializer='zeros'        )(x1)\n",
        "        x1 = layers.ReLU()(x1)\n",
        "\n",
        "    q1_out = layers.Dense(\n",
        "        1,\n",
        "        kernel_initializer='glorot_uniform',  # Default Xavier/Glorot init\n",
        "        bias_initializer='zeros',\n",
        "        name='q1_out'\n",
        "    )(x1)\n",
        "    q1_model = keras.Model([s_in, a_in], q1_out, name='Q1')\n",
        "    x2 = concat_in  # Remove BatchNorm here\n",
        "    # Q2 Network (separate network with same architecture)\n",
        "    for size in hidden_sizes:\n",
        "        x2 = layers.Dense(\n",
        "            size,\n",
        "            kernel_initializer='glorot_uniform',  # Default Xavier/Glorot init\n",
        "            bias_initializer='zeros',\n",
        "        )(x2)\n",
        "        x2 = layers.ReLU()(x2)\n",
        "\n",
        "    q2_out = layers.Dense(\n",
        "        1,\n",
        "        kernel_initializer='glorot_uniform',  # Default Xavier/Glorot init\n",
        "        bias_initializer='zeros',\n",
        "        name='q2_out'\n",
        "    )(x2)\n",
        "    q2_model = keras.Model([s_in, a_in], q2_out, name='Q2')\n",
        "\n",
        "    return q1_model, q2_model\n",
        "\n",
        "def create_policy_network(state_dim, action_dim, hidden_sizes, max_action):\n",
        "    s_in = keras.Input(shape=(state_dim,), dtype=tf.float32)\n",
        "    x = s_in\n",
        "\n",
        "    # Hidden layers\n",
        "    for size in hidden_sizes:\n",
        "        x = layers.Dense(\n",
        "            size,\n",
        "            activation='relu',\n",
        "            kernel_initializer=keras.initializers.HeUniform()  # Change to He init\n",
        "        )(x)\n",
        "\n",
        "    # Mean should start very small\n",
        "    mean = layers.Dense(\n",
        "        action_dim,\n",
        "        kernel_initializer=keras.initializers.RandomUniform(-3e-3, 3e-3),  # Standard DDPG/SAC init\n",
        "        bias_initializer='zeros'\n",
        "    )(x)\n",
        "\n",
        "    # Log std should start negative\n",
        "    log_std = layers.Dense(\n",
        "        action_dim,\n",
        "        kernel_initializer=keras.initializers.RandomUniform(-3e-3, 3e-3),\n",
        "    )(x)\n",
        "\n",
        "    return keras.Model(inputs=s_in, outputs=[mean, log_std])\n",
        "\n",
        "\n",
        "def create_diff_network(state_dim, action_dim, hidden_sizes, name):\n",
        "    s_in = keras.Input(shape=(state_dim,), dtype=tf.float32)\n",
        "    a_in = keras.Input(shape=(action_dim,), dtype=tf.float32)\n",
        "    sn_in = keras.Input(shape=(state_dim,), dtype=tf.float32)\n",
        "\n",
        "    # Normalize inputs\n",
        "    concat_in = layers.Concatenate(axis=-1)([s_in, a_in, sn_in])\n",
        "\n",
        "    x = concat_in\n",
        "    # Hidden layers with proper initialization\n",
        "    for size in hidden_sizes:\n",
        "        x = layers.Dense(\n",
        "            size,\n",
        "            activation='relu',\n",
        "            kernel_initializer='glorot_uniform',  # Default Xavier/Glorot init\n",
        "            bias_initializer='zeros',\n",
        "        )(x)\n",
        "\n",
        "    # Small init for final layer\n",
        "    diff_out = layers.Dense(\n",
        "        1,\n",
        "        activation=None,\n",
        "        kernel_initializer=keras.initializers.orthogonal(0.01),\n",
        "        bias_initializer='zeros'\n",
        "    )(x)\n",
        "\n",
        "    model = keras.Model([s_in, a_in, sn_in], diff_out, name=name)\n",
        "    return model\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 3. The ReshuffleSACAgent (no architecture changes)\n",
        "# ----------------------------------------------------\n",
        "class ReshuffleSACAgent:\n",
        "    def __init__(self, env, config: Config):\n",
        "        self.env = env\n",
        "        self.config = config\n",
        "        if config.with_lagrange:\n",
        "            self.log_alpha_prime_rs = tf.Variable(0.0)\n",
        "            self.alpha_prime_optimizer_rs = tf.keras.optimizers.Adam(config.q_lr)\n",
        "            self.log_alpha_prime_d = tf.Variable(0.0)\n",
        "            self.alpha_prime_optimizer_d = tf.keras.optimizers.Adam(config.q_lr)\n",
        "        obs_dim = env.observation_space.shape[0]\n",
        "        act_dim = env.action_space.shape[0]\n",
        "        self.max_action = float(env.action_space.high[0])\n",
        "        self.state_dim = obs_dim\n",
        "        self.action_dim = act_dim\n",
        "\n",
        "        self.global_step = tf.Variable(0, dtype=tf.int64)\n",
        "\n",
        "        # Critics\n",
        "        self.Q_RS1, self.Q_RS2 = create_double_q_networks(obs_dim, act_dim, config.hidden_sizes)\n",
        "        self.Q_D1, self.Q_D2   = create_double_q_networks(obs_dim, act_dim, config.hidden_sizes)\n",
        "        self.target_Q_RS1, self.target_Q_RS2 = create_double_q_networks(obs_dim, act_dim, config.hidden_sizes)\n",
        "        self.target_Q_D1,  self.target_Q_D2  = create_double_q_networks(obs_dim, act_dim, config.hidden_sizes)\n",
        "        self.Q_RS1_old, self.Q_RS2_old = create_double_q_networks(obs_dim, act_dim, config.hidden_sizes)\n",
        "        self.Q_D1_old,  self.Q_D2_old  = create_double_q_networks(obs_dim, act_dim, config.hidden_sizes)\n",
        "        self.target_Q_RS1_old, self.target_Q_RS2_old = create_double_q_networks(obs_dim, act_dim, config.hidden_sizes)\n",
        "        self.target_Q_D1_old,  self.target_Q_D2_old  = create_double_q_networks(obs_dim, act_dim, config.hidden_sizes)\n",
        "\n",
        "        # Actor & difference networks\n",
        "        self.actor = create_policy_network(obs_dim, act_dim, config.hidden_sizes, self.max_action)\n",
        "        self.N1 = create_diff_network(obs_dim, act_dim, config.hidden_sizes, \"N1\")\n",
        "        self.N2 = create_diff_network(obs_dim, act_dim, config.hidden_sizes, \"N2\")\n",
        "\n",
        "        # Alpha\n",
        "        if config.target_entropy is None:\n",
        "         self.config.target_entropy = -6\n",
        "        self.log_alpha_rs = tf.Variable(np.log(config.init_alpha_rs), dtype=tf.float32)\n",
        "        self.log_alpha_d = tf.Variable(np.log(config.init_alpha_d), dtype=tf.float32)\n",
        "\n",
        "        # Sync\n",
        "        self.sync_networks_hard()\n",
        "        self.sync_old_networks()\n",
        "\n",
        "        # LR schedules\n",
        "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "            initial_learning_rate=config.initial_learning_rate,\n",
        "            decay_steps=20000,\n",
        "            decay_rate=0.99,\n",
        "            staircase=True\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        # In ReshuffleSACAgent init:\n",
        "        self.actor_d = create_policy_network(obs_dim, act_dim, config.hidden_sizes, self.max_action)\n",
        "\n",
        "        self.q_rs1_opt = keras.optimizers.Adam(lr_schedule)\n",
        "        self.q_rs2_opt = keras.optimizers.Adam(lr_schedule)\n",
        "        self.q_d1_opt = keras.optimizers.Adam(lr_schedule)\n",
        "        self.q_d2_opt = keras.optimizers.Adam(lr_schedule)\n",
        "\n",
        "        self.actor_opt = keras.optimizers.Adam(lr_schedule)\n",
        "        self.actor_d_opt = keras.optimizers.Adam(lr_schedule)\n",
        "\n",
        "        self.alpha_rs_opt = keras.optimizers.Adam(config.alpha_lr)\n",
        "        self.alpha_d_opt = keras.optimizers.Adam(config.alpha_lr)\n",
        "        self.n1_opt    = keras.optimizers.Adam(lr_schedule)\n",
        "        self.n2_opt    = keras.optimizers.Adam(lr_schedule)\n",
        "\n",
        "        # Tau override\n",
        "        self.tau_override = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
        "    @tf.function\n",
        "    def compute_cql_penalty(\n",
        "        self,\n",
        "        q_network,\n",
        "        states,\n",
        "        next_states,\n",
        "        actions,\n",
        "        q_pred,            # Q(s, a_data)\n",
        "        log_alpha_prime,\n",
        "        sample_fn\n",
        "    ):\n",
        "        \"\"\"\n",
        "        A corrected version that avoids shape errors and closely\n",
        "        follows the rlkit-style CQL (random + data + policy + next).\n",
        "        \"\"\"\n",
        "        batch_size = tf.shape(states)[0]                # e.g. 256\n",
        "        act_dim    = tf.shape(actions)[1]\n",
        "        num_rand   = self.config.num_random            # e.g. 10\n",
        "\n",
        "        # ------------------------------------------------------------\n",
        "        # 1) Random actions (sampled from Uniform[-1,1]) for s\n",
        "        # ------------------------------------------------------------\n",
        "        # We'll tile 'states' so it has shape [batch_size*num_rand, obs_dim].\n",
        "        repeated_states = tf.repeat(states, repeats=num_rand, axis=0)\n",
        "        # random actions => shape [batch_size*num_rand, act_dim]\n",
        "        random_actions = tf.random.uniform(\n",
        "            shape=(batch_size * num_rand, act_dim),\n",
        "            minval=-1.0,\n",
        "            maxval=1.0\n",
        "        )\n",
        "        # Evaluate Q(s, a_rand)\n",
        "        q_rand = q_network([repeated_states, random_actions])  # => [batch_size*num_rand, 1]\n",
        "        q_rand = tf.reshape(q_rand, [batch_size, num_rand])    # => [batch_size, num_rand]\n",
        "\n",
        "        # Optionally subtract \"random density\" if you want importance sampling\n",
        "        random_density = tf.cast(act_dim, tf.float32) * tf.math.log(tf.constant(0.5, dtype=tf.float32))\n",
        "        q_rand_adjusted = q_rand - random_density\n",
        "        # but let's keep it as q_rand if you prefer\n",
        "\n",
        "        # ------------------------------------------------------------\n",
        "        # 2) Data action Q(s, a_data) is given by q_pred\n",
        "        #    We'll keep it shaped [batch_size, 1].\n",
        "        # ------------------------------------------------------------\n",
        "        q_data = tf.reshape(q_pred, [batch_size, 1])\n",
        "\n",
        "        # ------------------------------------------------------------\n",
        "        # 3) Current policy actions from 'states'\n",
        "        # ------------------------------------------------------------\n",
        "        # We'll sample 'num_rand' times for each s => tile states again\n",
        "        repeated_states2 = tf.repeat(states, repeats=num_rand, axis=0)\n",
        "        curr_actions, curr_log_pis = sample_fn(repeated_states2)\n",
        "        # Evaluate Q\n",
        "        q_curr = q_network([repeated_states2, curr_actions])    # => [batch_size*num_rand, 1]\n",
        "        # Reshape to [batch_size, num_rand]\n",
        "        q_curr = tf.reshape(q_curr, [batch_size, num_rand])\n",
        "        curr_log_pis = tf.reshape(curr_log_pis, [batch_size, num_rand])\n",
        "        # We'll do (Q - log_pi)\n",
        "        q_curr_adjusted = q_curr - curr_log_pis\n",
        "\n",
        "        # ------------------------------------------------------------\n",
        "        # 4) Next policy actions from next_states (tile them too),\n",
        "        #    then evaluate Q(s, a_next). This matches rlkit's approach:\n",
        "        #    next_actions come from pi(...| s'), but still feed into Q(s, a_next).\n",
        "        # ------------------------------------------------------------\n",
        "        repeated_next_states = tf.repeat(next_states, repeats=num_rand, axis=0)\n",
        "        next_actions_samps, next_log_pis = sample_fn(repeated_next_states)\n",
        "        # Evaluate Q(s, a_next) => we feed 'repeated_states' or 'repeated_states2'?\n",
        "        # We'll use repeated_states here so it's Q(s, a_{next}).\n",
        "        # Must be consistent in shape => repeated_states as well:\n",
        "        q_next = q_network([repeated_next_states, next_actions_samps])\n",
        "        q_next = tf.reshape(q_next, [batch_size, num_rand])         # => [batch_size, num_rand]\n",
        "        next_log_pis = tf.reshape(next_log_pis, [batch_size, num_rand])\n",
        "        q_next_adjusted = q_next - next_log_pis\n",
        "\n",
        "        # ------------------------------------------------------------\n",
        "        # 5) Concatenate all candidate Q-values: random, data, curr, next\n",
        "        # ------------------------------------------------------------\n",
        "        cat_q = tf.concat(\n",
        "            [\n",
        "                q_data,           # shape [batch_size, 1]\n",
        "                q_rand_adjusted,           # or q_rand_adjusted\n",
        "                q_curr_adjusted,\n",
        "                q_next_adjusted\n",
        "            ],\n",
        "            axis=1\n",
        "        ) # => shape [batch_size, 1 + 3*num_rand]\n",
        "\n",
        "        # ------------------------------------------------------------\n",
        "        # 6) Single log-sum-exp across that dimension => [batch_size]\n",
        "        # ------------------------------------------------------------\n",
        "        cql_logsumexp = tf.reduce_logsumexp(cat_q / self.config.temp, axis=1)\n",
        "        cql_logsumexp = cql_logsumexp * self.config.temp\n",
        "\n",
        "        # 7) Subtract data Q\n",
        "        cql_loss = cql_logsumexp - tf.squeeze(q_data, axis=1)  # subtract shape [batch_size]\n",
        "\n",
        "        # 8) Mean across batch\n",
        "        cql_loss = tf.reduce_mean(cql_loss)\n",
        "\n",
        "        # 9) Multiply by min_q_weight\n",
        "        cql_loss *= self.config.min_q_weight\n",
        "\n",
        "        # 10) If using Lagrange version\n",
        "        if self.config.with_lagrange:\n",
        "            alpha_prime = tf.clip_by_value(tf.exp(log_alpha_prime), 0.0, 1.0e6)\n",
        "            lagrange_obj = cql_loss - self.config.lagrange_thresh\n",
        "            cql_loss = alpha_prime * lagrange_obj\n",
        "            alpha_prime_loss = -cql_loss  # we minimize => maximizing the negative\n",
        "            return cql_loss, alpha_prime_loss\n",
        "\n",
        "        return cql_loss\n",
        "\n",
        "\n",
        "    def act_d(self, state, deterministic=False):\n",
        "        state_tf = tf.convert_to_tensor(state.reshape(1,-1), dtype=tf.float32)\n",
        "        mean, log_std = self.actor_d(state_tf)\n",
        "        log_std = tf.clip_by_value(log_std, -20, 2)\n",
        "        std = tf.exp(log_std)\n",
        "        if deterministic:\n",
        "            return (tf.tanh(mean) * self.max_action)[0].numpy()\n",
        "        eps = tf.random.normal(shape=mean.shape)\n",
        "        pre_tanh = mean + std * eps\n",
        "        return (tf.tanh(pre_tanh) * self.max_action)[0].numpy()\n",
        "    def sync_networks_hard(self):\n",
        "        self.target_Q_RS1.set_weights(self.Q_RS1.get_weights())\n",
        "        self.target_Q_RS2.set_weights(self.Q_RS2.get_weights())\n",
        "        self.target_Q_D1.set_weights(self.Q_D1.get_weights())\n",
        "        self.target_Q_D2.set_weights(self.Q_D2.get_weights())\n",
        "    @tf.function\n",
        "    def policy_diagnosis(self, states):\n",
        "        \"\"\"Diagnose policy behavior\"\"\"\n",
        "        mean, log_std = self.actor(states)\n",
        "        std = tf.exp(tf.clip_by_value(log_std, -20, 2))\n",
        "        tf.print(\"\\nPolicy Diagnostics:\")\n",
        "        tf.print(\"Mean range:\", tf.reduce_min(mean), tf.reduce_max(mean))\n",
        "        tf.print(\"Std range:\", tf.reduce_min(std), tf.reduce_max(std))\n",
        "\n",
        "        actions, _ = self.sample_actions_logp(states)\n",
        "        tf.print(\"Action range:\", tf.reduce_min(actions), tf.reduce_max(actions))\n",
        "\n",
        "        q1 = self.Q_RS1([states, actions])\n",
        "        q2 = self.Q_RS2([states, actions])\n",
        "        tf.print(\"Q-values range:\", tf.reduce_min(q1), tf.reduce_max(q1))\n",
        "\n",
        "        return actions\n",
        "\n",
        "    @tf.function\n",
        "    def value_diagnosis(self, states, actions):\n",
        "        \"\"\"Diagnose value estimation\"\"\"\n",
        "        tf.print(\"\\nValue Diagnostics:\")\n",
        "        # Current Q-values\n",
        "        q_rs1 = self.Q_RS1([states, actions])\n",
        "        q_rs2 = self.Q_RS2([states, actions])\n",
        "        q_d1 = self.Q_D1([states, actions])\n",
        "        q_d2 = self.Q_D2([states, actions])\n",
        "\n",
        "        tf.print(\"Q_RS1 stats:\", tf.reduce_min(q_rs1), tf.reduce_mean(q_rs1), tf.reduce_max(q_rs1))\n",
        "        tf.print(\"Q_RS2 stats:\", tf.reduce_min(q_rs2), tf.reduce_mean(q_rs2), tf.reduce_max(q_rs2))\n",
        "        tf.print(\"Q_D1 stats:\", tf.reduce_min(q_d1), tf.reduce_mean(q_d1), tf.reduce_max(q_d1))\n",
        "        tf.print(\"Q_D2 stats:\", tf.reduce_min(q_d2), tf.reduce_mean(q_d2), tf.reduce_max(q_d2))\n",
        "\n",
        "        # Target Q-values\n",
        "        next_actions, next_logp = self.sample_actions_logp(states)\n",
        "        target_q1 = self.target_Q_RS1([states, next_actions])\n",
        "        target_q2 = self.target_Q_RS2([states, next_actions])\n",
        "\n",
        "        tf.print(\"Target Q stats:\", tf.reduce_min(target_q1), tf.reduce_mean(target_q1), tf.reduce_max(target_q1))\n",
        "        tf.print(\"Next logp stats:\", tf.reduce_min(next_logp), tf.reduce_mean(next_logp), tf.reduce_max(next_logp))\n",
        "\n",
        "    def sync_old_networks(self):\n",
        "        self.Q_RS1_old.set_weights(self.Q_RS1.get_weights())\n",
        "        self.Q_RS2_old.set_weights(self.Q_RS2.get_weights())\n",
        "        self.Q_D1_old.set_weights(self.Q_D1.get_weights())\n",
        "        self.Q_D2_old.set_weights(self.Q_D2.get_weights())\n",
        "        self.target_Q_RS1_old.set_weights(self.target_Q_RS1.get_weights())\n",
        "        self.target_Q_RS2_old.set_weights(self.target_Q_RS2.get_weights())\n",
        "        self.target_Q_D1_old.set_weights(self.target_Q_D1.get_weights())\n",
        "        self.target_Q_D2_old.set_weights(self.target_Q_D2.get_weights())\n",
        "\n",
        "    def soft_update(self, source_vars, target_vars):\n",
        "        tau = self.config.soft_update_tau\n",
        "        for s_var, t_var in zip(source_vars, target_vars):\n",
        "            t_var.assign(tau * s_var + (1 - tau) * t_var)\n",
        "\n",
        "    def soft_update_all(self):\n",
        "        self.soft_update(self.Q_RS1.trainable_variables, self.target_Q_RS1.trainable_variables)\n",
        "        self.soft_update(self.Q_RS2.trainable_variables, self.target_Q_RS2.trainable_variables)\n",
        "        self.soft_update(self.Q_D1.trainable_variables,  self.target_Q_D1.trainable_variables)\n",
        "        self.soft_update(self.Q_D2.trainable_variables,  self.target_Q_D2.trainable_variables)\n",
        "\n",
        "    def act(self, state, deterministic=False):\n",
        "        state_tf = tf.convert_to_tensor(state.reshape(1,-1), dtype=tf.float32)\n",
        "        mean, log_std = self.actor(state_tf)\n",
        "        log_std = tf.clip_by_value(log_std, -20, 2)\n",
        "        std = tf.exp(log_std)\n",
        "        if deterministic:\n",
        "            return (tf.tanh(mean) * self.max_action)[0].numpy()\n",
        "        eps = tf.random.normal(shape=mean.shape)\n",
        "        pre_tanh = mean + std * eps\n",
        "        return (tf.tanh(pre_tanh) * self.max_action)[0].numpy()\n",
        "\n",
        "    @tf.function\n",
        "    def sample_actions_logp(self, states):\n",
        "        mean, log_std = self.actor(states)\n",
        "        std = tf.exp(tf.clip_by_value(log_std, -20, 2))\n",
        "\n",
        "        # Create normal distribution\n",
        "        distribution = tfp.distributions.Normal(mean, std)\n",
        "\n",
        "        # Sample raw actions\n",
        "        raw_actions = distribution.sample()\n",
        "\n",
        "        # Squash actions and calculate log probs\n",
        "        actions = tf.tanh(raw_actions)\n",
        "        log_prob = distribution.log_prob(raw_actions)\n",
        "\n",
        "        # Apply tanh correction\n",
        "        log_prob -= tf.reduce_sum(tf.math.log(1.0 - tf.square(actions) + 1e-6), axis=1, keepdims=True)\n",
        "        log_prob = tf.reduce_sum(log_prob, axis=1, keepdims=True)\n",
        "\n",
        "        return actions * self.max_action, log_prob\n",
        "\n",
        "    # Same for the D policy\n",
        "    @tf.function\n",
        "    def sample_actions_logp_d(self, states):\n",
        "        mean, log_std = self.actor_d(states)\n",
        "        std = tf.exp(tf.clip_by_value(log_std, -20, 2))\n",
        "\n",
        "        distribution = tfp.distributions.Normal(mean, std)\n",
        "        raw_actions = distribution.sample()\n",
        "        actions = tf.tanh(raw_actions)\n",
        "        log_prob = distribution.log_prob(raw_actions)\n",
        "        log_prob -= tf.reduce_sum(tf.math.log(1.0 - tf.square(actions) + 1e-6), axis=1, keepdims=True)\n",
        "        log_prob = tf.reduce_sum(log_prob, axis=1, keepdims=True)\n",
        "\n",
        "        return actions * self.max_action, log_prob\n",
        "    # GMM + PCA for p^D\n",
        "    def fit_behavior_gmm(self, all_states, all_actions, pca_dim=15):\n",
        "        total_samples = all_states.shape[0]\n",
        "        subset_size = min(100000, total_samples)\n",
        "        idxes = np.random.choice(total_samples, size=subset_size, replace=False)\n",
        "        states_subset = all_states[idxes]\n",
        "        actions_subset = all_actions[idxes]\n",
        "\n",
        "        X = np.concatenate([states_subset, actions_subset], axis=1)\n",
        "        self.pca = PCA(n_components=pca_dim)\n",
        "        X_reduced = self.pca.fit_transform(X)\n",
        "        self.gmm = GaussianMixture(\n",
        "            n_components=self.config.gmm_num_components,\n",
        "            max_iter=self.config.gmm_max_iter,\n",
        "            covariance_type='full',\n",
        "            verbose=2\n",
        "        )\n",
        "        start = time.time()\n",
        "        self.gmm.fit(X_reduced)\n",
        "        print(f\"GMM fit done in {time.time()-start:.1f} sec. Using {subset_size} samples.\")\n",
        "\n",
        "    def eval_p_d(self, states, actions):\n",
        "        X = np.concatenate([states, actions], axis=-1)\n",
        "        X_reduced = self.pca.transform(X)\n",
        "        logp = self.gmm.score_samples(X_reduced)\n",
        "        return np.float32(np.exp(logp))\n",
        "\n",
        "    @tf.function\n",
        "    def sac_backup(self, next_states, rewards, dones, target_q1, target_q2, alpha_val):\n",
        "        next_acts, next_logp = self.sample_actions_logp(next_states)\n",
        "        q1_next = target_q1([next_states, next_acts])\n",
        "        q2_next = target_q2([next_states, next_acts])\n",
        "        min_q_next = tf.minimum(q1_next, q2_next)\n",
        "        backup = rewards + self.config.gamma * (1.0 - dones) * (min_q_next - alpha_val  * next_logp)\n",
        "        return backup\n",
        "    @tf.function\n",
        "    def sac_backup_d(self, next_states, rewards, dones, target_q1, target_q2, alpha_val):\n",
        "        # now sample from actor_d instead of actor\n",
        "        next_acts_d, next_logp_d = self.sample_actions_logp_d(next_states)\n",
        "        q1_next = target_q1([next_states, next_acts_d])\n",
        "        q2_next = target_q2([next_states, next_acts_d])\n",
        "        min_q_next = tf.minimum(q1_next, q2_next)\n",
        "        backup = rewards + self.config.gamma * (1.0 - dones) * (min_q_next - alpha_val  * next_logp_d)\n",
        "        return backup\n",
        "\n",
        "    def partial_blend_weights(self, uniform_w, discor_w, step_int):\n",
        "        # More gradual ramp: from warm_up to warm_up + ramp_up\n",
        "        warm = float(self.config.warm_up_steps)\n",
        "        ramp = float(self.config.ramp_up_steps)\n",
        "        step_f = tf.cast(step_int, tf.float32)\n",
        "        start_ramp = warm\n",
        "        end_ramp = warm + ramp  # e.g. 5000->15000\n",
        "\n",
        "        alpha = tf.clip_by_value((step_f - start_ramp)/(end_ramp - start_ramp), 0.0, 1.0)\n",
        "        return discor_w * alpha + uniform_w * (1 - alpha)\n",
        "\n",
        "    @tf.function\n",
        "    def calculate_weights(self, q_rs_vals, q_d_vals, p_d_batch, n1_pred, n2_pred):\n",
        "        \"\"\"\n",
        "        p_d_vals here are the raw GMM densities for the minibatch samples,\n",
        "        already properly normalized from the full GMM distribution\n",
        "        \"\"\"\n",
        "        tau = tf.cond(\n",
        "            self.tau_override > 0.0,\n",
        "            lambda: self.tau_override,\n",
        "            lambda: tf.cast(self.config.get_tau(self.global_step), tf.float32)\n",
        "        )\n",
        "\n",
        "        # Calculate log_p_rs as before\n",
        "        softened_n2 = 0.5 + 0.5 * tf.abs(n2_pred)\n",
        "        log_p_rs = -softened_n2 / tau + tf.math.log(tf.abs(n1_pred) + 1e-8)\n",
        "        # Get unnormalized p_rs\n",
        "        log_p_rs=tf.abs(log_p_rs)\n",
        "        numerator = log_p_rs\n",
        "        denom =  tfp.stats.percentile(numerator , 50.0)+ 1e-8\n",
        "        p_rs = numerator / denom  # This is your new distribution p_rs(s,a)\n",
        "\n",
        "                      # 1) Compute 99th percentile of ratio\n",
        "        #    (Requires e.g. 'tfp' or your own percentile code)\n",
        "        percentile_q = 99.0\n",
        "        # If you have tfp:\n",
        "        # import tensorflow_probability as tfp\n",
        "        ratio_percentile_val = tfp.stats.percentile(p_rs, q=99)\n",
        "        # Or roll your own percentile with tf.sort(...)\n",
        "\n",
        "        # Quick approximate percentile:\n",
        "\n",
        "\n",
        "        # 2) Clip\n",
        "        ratio_clipped = tf.minimum(p_rs, ratio_percentile_val)\n",
        "        ratio_percentile_val = tfp.stats.percentile(p_rs, q=1)\n",
        "\n",
        "        ratio_clipped = tf.maximum(ratio_clipped, ratio_percentile_val)\n",
        "\n",
        "        weights = ratio_clipped\n",
        "\n",
        "        # Optional: track KL\n",
        "        kl_div = tf.reduce_sum(\n",
        "            p_rs * (  # Use p_rs instead of ratio_standard\n",
        "                tf.math.log(p_rs + 1e-8)\n",
        "                - tf.math.log(p_d_batch + 1e-8)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return weights, kl_div, tau\n",
        "\n",
        "\n",
        "    # Add to training loop:\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def get_log_prob_of_data(self,actor, states, data_actions, max_action):\n",
        "        \"\"\"\n",
        "        Compute log pi(a_data | s) for your Tanh-Gaussian policy 'actor'.\n",
        "\n",
        "        :param actor: e.g. self.actor\n",
        "        :param states: shape [batch, obs_dim]\n",
        "        :param data_actions: shape [batch, act_dim],\n",
        "                            assumed in [-max_action, max_action].\n",
        "        :param max_action: float (e.g. self.max_action)\n",
        "\n",
        "        :return: log_probs, shape [batch, 1]\n",
        "        \"\"\"\n",
        "        # 1) 'inverse tanh' for data_actions in [-max_action, max_action].\n",
        "        #    If your environment always has actions in [-1,1],\n",
        "        #    you can do data_actions_clamped = tf.clip_by_value(data_actions/max_action, -0.999, 0.999)\n",
        "        #    or just assume they’re in [-1,1].\n",
        "        scaled_actions = data_actions / max_action\n",
        "        # Tanh is defined for input in (-1,1). So clamp slightly:\n",
        "        clipped_acts = tf.clip_by_value(scaled_actions, -0.999999, 0.999999)\n",
        "        raw_a_data = 0.5 * tf.math.log((1.0 + clipped_acts)/(1.0 - clipped_acts))  # atanh\n",
        "\n",
        "        # 2) Get policy's (mu, log_std) for the states\n",
        "        mean, log_std = actor(states)\n",
        "        log_std = tf.clip_by_value(log_std, -20, 2)\n",
        "        std = tf.exp(log_std)\n",
        "\n",
        "        # 3) Evaluate normal pdf at raw_a_data\n",
        "        distribution = tfp.distributions.Normal(loc=mean, scale=std)\n",
        "        # log_prob for each dim => shape [batch, act_dim]\n",
        "        logp_each_dim = distribution.log_prob(raw_a_data)\n",
        "\n",
        "        # 4) Tanh correction: we must subtract log(1 - tanh^2(...))\n",
        "        #   But we want log(1 - (tanh(raw_a))^2)\n",
        "        #   raw_a_data was exactly \"the inside\" of tanh\n",
        "        # So:\n",
        "        log_det_jac = tf.reduce_sum(\n",
        "            tf.math.log(1.0 - tf.square(tf.tanh(raw_a_data)) + 1e-6),\n",
        "            axis=1, keepdims=True\n",
        "        )\n",
        "\n",
        "        # 5) Sum across action dims => shape [batch, 1]\n",
        "        log_prob = tf.reduce_sum(logp_each_dim, axis=1, keepdims=True)\n",
        "\n",
        "        # 6) Subtract the Jacobian\n",
        "        log_prob = log_prob - log_det_jac\n",
        "\n",
        "        return log_prob\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, states, actions, rewards, next_states, dones, p_d_vals):\n",
        "        alpha_rs_val = tf.exp(self.log_alpha_rs)\n",
        "        alpha_d_val = tf.exp(self.log_alpha_d)\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # Evaluate Q\n",
        "            q_rs1_vals = self.Q_RS1([states, actions])[:, 0]\n",
        "            q_rs2_vals = self.Q_RS2([states, actions])[:, 0]\n",
        "            q_d1_vals  = self.Q_D1([states, actions])[:, 0]\n",
        "            q_d2_vals  = self.Q_D2([states, actions])[:, 0]\n",
        "            q_rs_vals  = tf.minimum(q_rs1_vals, q_rs2_vals)\n",
        "            q_d_vals   = tf.minimum(q_d1_vals,  q_d2_vals)\n",
        "\n",
        "            n1_pred = self.N1([states, actions, next_states])[:, 0]\n",
        "            n2_pred = self.N2([states, actions, next_states])[:, 0]\n",
        "\n",
        "            # Uniform vs DisCor\n",
        "            batch_size = tf.shape(q_rs_vals)[0]\n",
        "            uniform_w = tf.ones((batch_size,), dtype=tf.float32)\n",
        "            discor_w, kl_div, tau_used = self.calculate_weights(q_rs_vals, q_d_vals, p_d_vals[:,0], n1_pred, n2_pred)\n",
        "\n",
        "            step_int = self.global_step\n",
        "            def combined_weights():\n",
        "                return self.partial_blend_weights(uniform_w, discor_w, step_int), kl_div, tau_used\n",
        "\n",
        "            w_cond1 = (step_int < self.config.warm_up_steps)\n",
        "            weights, kl_final, tau_final = tf.cond(\n",
        "                w_cond1,\n",
        "                lambda: (uniform_w, tf.constant(0.0, dtype=tf.float32), tf.constant(0.0, dtype=tf.float32)),\n",
        "                lambda: combined_weights()\n",
        "            )\n",
        "\n",
        "            # If KL is large, gradually raise tau\n",
        "\n",
        "\n",
        "            # SAC backups\n",
        "            bq_rs1 = self.sac_backup(next_states, rewards, dones, self.target_Q_RS1, self.target_Q_RS2, alpha_rs_val)\n",
        "            bq_rs2 = bq_rs1\n",
        "            bq_d1  = self.sac_backup_d(next_states, rewards, dones, self.target_Q_D1,  self.target_Q_D2,  alpha_d_val)\n",
        "            bq_d2  = bq_d1\n",
        "\n",
        "            old_q_rs1 = self.Q_RS1_old([states, actions])[:, 0]\n",
        "            old_q_rs2 = self.Q_RS2_old([states, actions])[:, 0]\n",
        "            old_q_rs  = tf.minimum(old_q_rs1, old_q_rs2)\n",
        "            bq_rs_old = self.sac_backup(next_states, rewards, dones, self.target_Q_RS1_old, self.target_Q_RS2_old, alpha_rs_val)\n",
        "\n",
        "            # Weighted MSE for Q_RS\n",
        "            bellman_loss_rs1 = tf.nn.compute_average_loss(\n",
        "                weights * tf.square(bq_rs1[:,0] - q_rs1_vals)\n",
        "            )\n",
        "            bellman_loss_rs2 = tf.nn.compute_average_loss(\n",
        "                weights * tf.square(bq_rs2[:,0] - q_rs2_vals)\n",
        "            )\n",
        "\n",
        "            cpen_rs1 = self.compute_cql_penalty(\n",
        "                self.Q_RS1, states, next_states, actions, q_rs1_vals,\n",
        "                None,  # not used\n",
        "                self.sample_actions_logp\n",
        "            )\n",
        "            cpen_rs2 = self.compute_cql_penalty(\n",
        "                self.Q_RS2, states, next_states, actions, q_rs2_vals,\n",
        "                None,\n",
        "                self.sample_actions_logp\n",
        "            )\n",
        "            # no alpha_prime_loss => so just 0.0 or skip\n",
        "            cpen_d1 = self.compute_cql_penalty(\n",
        "                self.Q_D1, states, next_states, actions, q_d1_vals,\n",
        "                None,\n",
        "                self.sample_actions_logp_d\n",
        "            )\n",
        "            cpen_d2 = self.compute_cql_penalty(\n",
        "                self.Q_D2, states, next_states, actions, q_d2_vals,\n",
        "                None,\n",
        "                self.sample_actions_logp_d\n",
        "            )\n",
        "\n",
        "            loss_q_rs1 = bellman_loss_rs1 + cpen_rs1\n",
        "            loss_q_rs2 = bellman_loss_rs2 + cpen_rs2\n",
        "            total_q_rs_loss = loss_q_rs1 + loss_q_rs2\n",
        "\n",
        "            # Q_D MSE\n",
        "            bellman_loss_d1 = tf.nn.compute_average_loss(\n",
        "                tf.square(bq_d1[:,0] - q_d1_vals)\n",
        "            )\n",
        "            bellman_loss_d2 = tf.nn.compute_average_loss(\n",
        "                tf.square(bq_d2[:,0] - q_d2_vals)\n",
        "            )\n",
        "            loss_q_d = bellman_loss_d1 + bellman_loss_d2+cpen_d2+cpen_d1\n",
        "\n",
        "            # N1 => Q_RS - BQ_RS_old\n",
        "            n1_target = q_rs_vals - bq_rs_old[:,0]\n",
        "            loss_n1 = tf.reduce_mean(tf.square(n1_target - n1_pred))\n",
        "\n",
        "            # N2 => Q_RS - Q_D\n",
        "            n2_target = q_rs_vals - q_d_vals\n",
        "            loss_n2 = tf.reduce_mean(tf.square(n2_target - n2_pred))\n",
        "\n",
        "        # ---- Gradient Clipping: Q and difference networks ----\n",
        "        # Critic Q_RS\n",
        "        q_rs_vars1 = self.Q_RS1.trainable_variables\n",
        "        q_rs_vars2 = self.Q_RS2.trainable_variables\n",
        "        q_rs_grads = tape.gradient(total_q_rs_loss, q_rs_vars1 + q_rs_vars2)\n",
        "        q_rs_grads, _ = tf.clip_by_global_norm(q_rs_grads, 10.0)  # clip norm=10\n",
        "        self.q_rs1_opt.apply_gradients(zip(q_rs_grads[:len(q_rs_vars1)], q_rs_vars1))\n",
        "        self.q_rs2_opt.apply_gradients(zip(q_rs_grads[len(q_rs_vars1):], q_rs_vars2))\n",
        "\n",
        "        # Critic Q_D\n",
        "        q_d_vars1 = self.Q_D1.trainable_variables\n",
        "        q_d_vars2 = self.Q_D2.trainable_variables\n",
        "        q_d_grads = tape.gradient(loss_q_d, q_d_vars1 + q_d_vars2)\n",
        "        q_d_grads, _ = tf.clip_by_global_norm(q_d_grads, 10.0)\n",
        "        self.q_d1_opt.apply_gradients(zip(q_d_grads[:len(q_d_vars1)], q_d_vars1))\n",
        "        self.q_d2_opt.apply_gradients(zip(q_d_grads[len(q_d_vars1):], q_d_vars2))\n",
        "\n",
        "        # Difference networks\n",
        "        n1_vars = self.N1.trainable_variables\n",
        "        n2_vars = self.N2.trainable_variables\n",
        "        n1_grads = tape.gradient(loss_n1, n1_vars)\n",
        "        n1_grads, _ = tf.clip_by_global_norm(n1_grads, 10.0)\n",
        "        self.n1_opt.apply_gradients(zip(n1_grads, n1_vars))\n",
        "\n",
        "        n2_grads = tape.gradient(loss_n2, n2_vars)\n",
        "        n2_grads, _ = tf.clip_by_global_norm(n2_grads, 10.0)\n",
        "        self.n2_opt.apply_gradients(zip(n2_grads, n2_vars))\n",
        "\n",
        "        del tape\n",
        "        # Suppose we store the current global training step in self.global_step\n",
        "        step_int = self.global_step\n",
        "\n",
        "        # Define placeholders for logging\n",
        "        actor_loss_rs = 0.0\n",
        "        actor_loss_d  = 0.0\n",
        "        alpha_rs_loss = 0.0\n",
        "        alpha_d_loss  = 0.0\n",
        "        if step_int >= self.config.policy_eval_start:\n",
        "            # ============= Normal CQL/SAC-style actor update =============\n",
        "            with tf.GradientTape(persistent=True) as actor_tape:\n",
        "                # RS policy\n",
        "                new_actions, logp = self.sample_actions_logp(states)   # shape [B, act_dim], [B,1]\n",
        "                q_rs1_pi = self.Q_RS1([states, new_actions])           # shape [B,1]\n",
        "                q_rs2_pi = self.Q_RS2([states, new_actions])\n",
        "                min_q_rs_pi = tf.minimum(q_rs1_pi, q_rs2_pi)\n",
        "\n",
        "                actor_loss_rs = tf.reduce_mean(alpha_rs_val * logp - min_q_rs_pi)\n",
        "\n",
        "                # D policy\n",
        "                new_actions_d, logp_d = self.sample_actions_logp_d(states)\n",
        "                q_d1_pi = self.Q_D1([states, new_actions_d])\n",
        "                q_d2_pi = self.Q_D2([states, new_actions_d])\n",
        "                min_q_d_pi = tf.minimum(q_d1_pi, q_d2_pi)\n",
        "\n",
        "                actor_loss_d = tf.reduce_mean(alpha_d_val * logp_d - min_q_d_pi)\n",
        "\n",
        "            actor_rs_grads = actor_tape.gradient(actor_loss_rs, self.actor.trainable_variables)\n",
        "            actor_d_grads  = actor_tape.gradient(actor_loss_d,  self.actor_d.trainable_variables)\n",
        "\n",
        "            actor_rs_grads, _ = tf.clip_by_global_norm(actor_rs_grads, 10.0)\n",
        "            actor_d_grads, _  = tf.clip_by_global_norm(actor_d_grads, 10.0)\n",
        "\n",
        "            self.actor_opt.apply_gradients(zip(actor_rs_grads, self.actor.trainable_variables))\n",
        "            self.actor_d_opt.apply_gradients(zip(actor_d_grads, self.actor_d.trainable_variables))\n",
        "            del actor_tape\n",
        "\n",
        "            # Now do alpha (temperature) updates\n",
        "            with tf.GradientTape() as alpha_rs_tape:\n",
        "                # typical: alpha_rs_loss = - E[ log_alpha_rs * (logp + target_entropy ) ]\n",
        "                alpha_rs_losses = -1.0 * (self.log_alpha_rs * tf.stop_gradient(logp + self.config.target_entropy))\n",
        "                alpha_rs_loss   = tf.nn.compute_average_loss(alpha_rs_losses)\n",
        "\n",
        "            alpha_rs_grads = alpha_rs_tape.gradient(alpha_rs_loss, [self.log_alpha_rs])\n",
        "            self.alpha_rs_opt.apply_gradients(zip(alpha_rs_grads, [self.log_alpha_rs]))\n",
        "\n",
        "            # same for alpha_d\n",
        "            with tf.GradientTape() as alpha_d_tape:\n",
        "                alpha_d_losses = -1.0 * (self.log_alpha_d * tf.stop_gradient(logp_d + self.config.target_entropy))\n",
        "                alpha_d_loss   = tf.nn.compute_average_loss(alpha_d_losses)\n",
        "\n",
        "            alpha_d_grads = alpha_d_tape.gradient(alpha_d_loss, [self.log_alpha_d])\n",
        "            self.alpha_d_opt.apply_gradients(zip(alpha_d_grads, [self.log_alpha_d]))\n",
        "\n",
        "        else:\n",
        "            # ============= Early phase: do BC objective =============\n",
        "            # For demonstration, let's do BC for both RS policy & D policy\n",
        "            # We have the offline dataset's (states, actions).\n",
        "            with tf.GradientTape(persistent=True) as bc_tape:\n",
        "                logprob_data_rs = self.get_log_prob_of_data(self.actor, states, actions, self.max_action)\n",
        "                bc_loss_rs      = -tf.reduce_mean(logprob_data_rs)  # negative log-likelihood\n",
        "\n",
        "                logprob_data_d  = self.get_log_prob_of_data(self.actor_d, states, actions, self.max_action)\n",
        "                bc_loss_d       = -tf.reduce_mean(logprob_data_d)\n",
        "\n",
        "            # If you want, you can add alpha * logp_data to bc_loss. Redwood-lab sometimes\n",
        "            # does: bc_loss = alpha * logp - log_prob_data, etc.\n",
        "            # But pure BC might be simpler.\n",
        "\n",
        "            bc_rs_grads = bc_tape.gradient(bc_loss_rs, self.actor.trainable_variables)\n",
        "            bc_d_grads  = bc_tape.gradient(bc_loss_d,  self.actor_d.trainable_variables)\n",
        "\n",
        "            bc_rs_grads, _ = tf.clip_by_global_norm(bc_rs_grads, 10.0)\n",
        "            bc_d_grads, _  = tf.clip_by_global_norm(bc_d_grads, 10.0)\n",
        "\n",
        "            self.actor_opt.apply_gradients(zip(bc_rs_grads, self.actor.trainable_variables))\n",
        "            self.actor_d_opt.apply_gradients(zip(bc_d_grads, self.actor_d.trainable_variables))\n",
        "            del bc_tape\n",
        "\n",
        "\n",
        "        # Soft update\n",
        "        if (self.global_step % self.config.target_update_freq) == 0:\n",
        "            self.soft_update_all()\n",
        "\n",
        "        bellman_err = 0.5*(bellman_loss_rs1 + bellman_loss_rs2)\n",
        "        bellman_err2 = 0.5*(bellman_loss_d1 + bellman_loss_d2)\n",
        "\n",
        "        value_err = tf.reduce_mean(tf.square(q_rs_vals - q_d_vals))\n",
        "        w_mean = tf.reduce_mean(weights)\n",
        "        self.global_step.assign_add(1)\n",
        "        alpha_loss=alpha_rs_loss\n",
        "        return {\n",
        "            \"loss_q_rs\": total_q_rs_loss,\n",
        "            \"loss_q_d\": loss_q_d,\n",
        "            \"loss_n1\": loss_n1,\n",
        "            \"loss_n2\": loss_n2,\n",
        "            \"kl_div\": kl_final,\n",
        "            \"actor_loss\": actor_loss_rs,\n",
        "            \"alpha_loss\": alpha_loss,\n",
        "            \"alpha_val\": alpha_rs_val,\n",
        "            \"bellman_err\": bellman_err,\n",
        "            \"bellman_err2\": bellman_err2,\n",
        "            \"value_err\": value_err,\n",
        "            \"weight_mean\": w_mean,\n",
        "            \"tau_used\": tau_final\n",
        "        }\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 4. Training Loop\n",
        "# ----------------------------------------------------\n",
        "def train_offline_sac_reshuffle(agent,env_id=\"HalfCheetah-v4\",\n",
        "                                seed=42,\n",
        "                                max_steps=35000,\n",
        "                                config=None,\n",
        "                                log_wandb=False):\n",
        "    if config is None:\n",
        "        config = Config()\n",
        "    set_seeds(seed)\n",
        "    # Initialize custom logger\n",
        "    logger = CustomLogger()\n",
        "\n",
        "    # Log configuration\n",
        "    config_dict = {\n",
        "        \"env_id\": env_id,\n",
        "        \"seed\": seed,\n",
        "        \"max_steps\": max_steps,\n",
        "        \"gamma\": config.gamma,\n",
        "        \"batch_size\": config.batch_size,\n",
        "        \"buffer_capacity\": config.buffer_capacity,\n",
        "        \"q_lr\": config.q_lr,\n",
        "        \"policy_lr\": config.policy_lr,\n",
        "        \"alpha_lr\": config.alpha_lr,\n",
        "        \"hidden_sizes\": config.hidden_sizes,\n",
        "        \"warm_up_steps\": config.warm_up_steps,\n",
        "        \"ramp_up_steps\": config.ramp_up_steps,\n",
        "        \"initial_tau\": config.initial_tau,\n",
        "        \"min_tau\": config.min_tau,\n",
        "        \"tau_decay\": config.tau_decay,\n",
        "        \"lambda_cql\": config.lambda_cql\n",
        "    }\n",
        "    logger.save_config(config_dict)\n",
        "    # new_step_api => remove old warnings\n",
        "    env = gym.make(env_id)\n",
        "\n",
        "    # Load offline dataset\n",
        "    with open('/content/drive/MyDrive/halfcheetah_medium_dataset.pkl', 'rb') as f:\n",
        "        dataset = pickle.load(f)\n",
        "\n",
        "    states_np = dataset['observations']\n",
        "    actions_np = dataset['actions']\n",
        "    rewards_np = dataset['rewards'].reshape(-1,1)\n",
        "\n",
        "\n",
        "    next_states_np = dataset['next_observations']\n",
        "    dones_np = dataset['terminals'].reshape(-1,1)\n",
        "    N = states_np.shape[0]\n",
        "    states_tf = tf.constant(dataset['observations'], dtype=tf.float32)\n",
        "    actions_tf = tf.constant(dataset['actions'], dtype=tf.float32)\n",
        "    rewards_tf = tf.constant(dataset['rewards'].reshape(-1,1), dtype=tf.float32)\n",
        "    next_states_tf = tf.constant(dataset['next_observations'], dtype=tf.float32)\n",
        "    dones_tf = tf.constant(dataset['terminals'].reshape(-1,1), dtype=tf.float32)\n",
        "\n",
        "    # Create TF dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        states_tf, actions_tf, rewards_tf, next_states_tf, dones_tf\n",
        "    )).shuffle(buffer_size=int(1e6)).batch(config.batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    agent = ReshuffleSACAgent(env, config)\n",
        "\n",
        "    # Fit GMM => p^D(s,a)\n",
        "\n",
        "    if log_wandb:\n",
        "\n",
        "      wandb_config = {\n",
        "          \"env_id\": env_id,\n",
        "          \"seed\": seed,\n",
        "          \"max_steps\": max_steps,\n",
        "          \"gamma\": config.gamma,\n",
        "          \"batch_size\": config.batch_size,\n",
        "          \"buffer_capacity\": config.buffer_capacity,\n",
        "          \"q_lr\": config.q_lr,\n",
        "          \"policy_lr\": config.policy_lr,\n",
        "          \"alpha_lr\": config.alpha_lr,\n",
        "          \"hidden_sizes\": config.hidden_sizes,\n",
        "          \"warm_up_steps\": config.warm_up_steps,\n",
        "          \"ramp_up_steps\": config.ramp_up_steps,\n",
        "          \"initial_tau\": config.initial_tau,\n",
        "          \"min_tau\": config.min_tau,\n",
        "          \"tau_decay\": config.tau_decay,\n",
        "          \"lambda_cql\": config.lambda_cql\n",
        "      }\n",
        "\n",
        "\n",
        "\n",
        "      wandb.init(\n",
        "      project=\"Offline-SAC-Reshuffle\",\n",
        "      name=f\"{env_id}-seed{seed}\",\n",
        "      config=wandb_config,\n",
        "      tags=[\"SAC\", \"Offline\", \"Reshuffle\"]\n",
        "      )\n",
        "\n",
        "    all_indices = np.arange(N)\n",
        "    steps_so_far = 0\n",
        "    gmm_fitted = False  # Add flag to track if GMM is fitted\n",
        "\n",
        "    while steps_so_far < max_steps:\n",
        "        # sync old networks at start of each epoch\n",
        "        agent.sync_old_networks()\n",
        "\n",
        "        for batch in dataset:\n",
        "            if steps_so_far >= max_steps:\n",
        "                break\n",
        "\n",
        "            s_batch, a_batch, r_batch, sn_batch, d_batch = batch\n",
        "\n",
        "\n",
        "            p_d_tf = tf.ones([s_batch.shape[0], 1], dtype=tf.float32)\n",
        "\n",
        "            metrics = agent.train_step(s_batch, a_batch, r_batch, sn_batch, d_batch, p_d_tf)\n",
        "            steps_so_far += 1\n",
        "            if log_wandb:\n",
        "                            wandb.log({\n",
        "                                \"step\": steps_so_far,\n",
        "                                \"loss/q_rs\": metrics['loss_q_rs'],\n",
        "                                \"loss/q_d\": metrics['loss_q_d'],\n",
        "                                \"loss/n1\": metrics['loss_n1'],\n",
        "                                \"loss/n2\": metrics['loss_n2'],\n",
        "                                \"metrics/kl_div\": metrics['kl_div'],\n",
        "                                \"loss/actor\": metrics['actor_loss'],\n",
        "                                \"loss/alpha\": metrics['alpha_loss'],\n",
        "                                \"metrics/alpha_value\": metrics['alpha_val'],\n",
        "                                \"metrics/bellman_error\": metrics['bellman_err'],\n",
        "                                 \"metrics/bellman_error2\": metrics['bellman_err2'],\n",
        "\n",
        "                                \"metrics/value_error\": metrics['value_err'],\n",
        "                                \"metrics/weight_mean\": metrics['weight_mean'],\n",
        "                                \"metrics/tau\": metrics['tau_used']\n",
        "                            })\n",
        "            # Log training metrics\n",
        "            if steps_so_far % 1000 == 0:\n",
        "                logger.log(metrics, steps_so_far)\n",
        "                print(f\"Step {steps_so_far} / {max_steps} => \"\n",
        "                      f\"Q_RS: {metrics['loss_q_rs']:.4f}, \"\n",
        "                      f\"Q_D: {metrics['loss_q_d']:.4f}, \"\n",
        "                      f\"N1: {metrics['loss_n1']:.4f}, \"\n",
        "                      f\"N2: {metrics['loss_n2']:.4f}, \"\n",
        "                      f\"Alpha: {metrics['alpha_val']:.4f}, \"\n",
        "                      f\"Actor: {metrics['actor_loss']:.4f}\")\n",
        "\n",
        "            if (steps_so_far % config.eval_interval) == 0:\n",
        "                eval_return_rs, eval_return_d = evaluate_both_policies(agent, env)\n",
        "                logger.log_eval({\n",
        "                    'return_rs': eval_return_rs,\n",
        "                    'return_d': eval_return_d\n",
        "                }, steps_so_far)\n",
        "                print(f\"[Eval] Step={steps_so_far}, RS Return={eval_return_rs:.2f}, D Return={eval_return_d:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "    return agent\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 5. Evaluation\n",
        "# ----------------------------------------------------\n",
        "def evaluate_offline_policy(agent: ReshuffleSACAgent, env, eval_episodes=5):\n",
        "    returns = []\n",
        "    for _ in range(eval_episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        ep_ret = 0\n",
        "        while not done:\n",
        "            act = agent.act(obs, deterministic=True)\n",
        "            obs_next, rew, done, info = env.step(act)\n",
        "            ep_ret += rew\n",
        "            obs = obs_next\n",
        "        returns.append(ep_ret)\n",
        "    return np.mean(returns)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 5. Evaluation\n",
        "# ----------------------------------------------------\n",
        "def evaluate_offline_policy(agent: ReshuffleSACAgent, env, eval_episodes=5):\n",
        "    returns = []\n",
        "    for _ in range(eval_episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        ep_ret = 0\n",
        "        while not done:\n",
        "            act = agent.act(obs, deterministic=True)\n",
        "            obs_next, rew, done, info = env.step(act)\n",
        "            ep_ret += rew\n",
        "            obs = obs_next\n",
        "        returns.append(ep_ret)\n",
        "    return np.mean(returns)\n",
        "def evaluate_both_policies(agent, env, eval_episodes=1):\n",
        "    returns_rs = []\n",
        "    returns_d = []\n",
        "\n",
        "    for _ in range(eval_episodes):\n",
        "        # Evaluate RS policy\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        ep_ret = 0\n",
        "        while not done:\n",
        "            act = agent.act(obs, deterministic=True)\n",
        "            obs_next, rew, done, info = env.step(act)\n",
        "            ep_ret += rew\n",
        "            obs = obs_next\n",
        "        returns_rs.append(ep_ret)\n",
        "\n",
        "        # Evaluate D policy\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        ep_ret = 0\n",
        "        while not done:\n",
        "            act = agent.act_d(obs, deterministic=True)\n",
        "            obs_next, rew, done, info = env.step(act)\n",
        "            ep_ret += rew\n",
        "            obs = obs_next\n",
        "        returns_d.append(ep_ret)\n",
        "\n",
        "    return np.mean(returns_rs), np.mean(returns_d)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 6. Main\n",
        "# ----------------------------------------------------\n",
        "cfg = Config()\n",
        "# Possibly override config if desired\n",
        "cfg.max_gradient_steps = 1000000\n",
        "\n",
        "env = gym.make(\"HalfCheetah-v4\")\n",
        "agent = ReshuffleSACAgent(env, cfg)\n",
        "\n",
        "agent = train_offline_sac_reshuffle(agent=agent,\n",
        "    env_id=\"HalfCheetah-v4\",\n",
        "    seed=42,\n",
        "    max_steps=cfg.max_gradient_steps,\n",
        "    config=cfg,\n",
        "    log_wandb=False\n",
        ")\n",
        "ret = evaluate_offline_policy(agent, env)\n",
        "print(f\"Final deterministic policy evaluation average return: {ret:.2f}\")\n"
      ],
      "metadata": {
        "id": "J4aHAxn3BWwH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}