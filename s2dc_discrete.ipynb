{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeZs+a6CZDjJ53zTx9WZwD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halduaij/S2DC-discrete/blob/main/s2dc_discrete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g11CEr9k_-sC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import tensorflow_probability as tfp\n",
        "reward_p=[]\n",
        "\n",
        "########################################\n",
        "# 1. Utilities and Config\n",
        "########################################\n",
        "\n",
        "def set_seeds(seed_value=38):\n",
        "    \"\"\"Ensure reproducibility across various libraries.\"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    tf.random.set_seed(seed_value)\n",
        "\n",
        "\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        # Core hyperparameters\n",
        "        self.gamma = 0.975           # Discount factor\n",
        "        self.batch_size = 256        # 1024*8\n",
        "        self.buffer_capacity = 10000\n",
        "        self.initial_learning_rate = 0.0001\n",
        "        self.n = int(np.ceil(np.sqrt(500)))  # 23 if we want to keep your n^2 logic\n",
        "\n",
        "        # Temperature parameters\n",
        "        self.initial_tau = 10.0\n",
        "        self.min_tau = 0.1\n",
        "        self.tau_decay = 0.999\n",
        "        self.max_tau = 20.0  # [IMPROVEMENT] - a maximum bound for adaptive tau\n",
        "\n",
        "        # Network update parameters\n",
        "        self.target_update_freq = 1\n",
        "        self.soft_update_tau = 0.005\n",
        "\n",
        "        # [IMPROVEMENT #1]: Additional hyperparams for difference networks in weighting\n",
        "        self.use_n1_in_weights = True\n",
        "        self.alpha = 0.1   # scale factor for incorporating N1 difference predictions\n",
        "\n",
        "        # [IMPROVEMENT #2]: Conservative Q penalty factor (CQL-like idea)\n",
        "        self.lambda_cql = 0.01\n",
        "\n",
        "        # [IMPROVEMENT #3]: Adaptive tau if KL becomes too large\n",
        "        self.kl_div_threshold = 50.0\n",
        "\n",
        "    def get_tau(self, step: int) -> float:\n",
        "        \"\"\"Get temperature parameter with a decay schedule.\"\"\"\n",
        "        # We'll default to an exponential decay, but\n",
        "        # the agent can override it adaptively if distribution is too peaked.\n",
        "        tau_raw = max(self.min_tau,\n",
        "                      self.initial_tau * (self.tau_decay ** (step // 1000)))\n",
        "        return tau_raw\n",
        "\n",
        "\n",
        "########################################\n",
        "# 2. Network Architectures\n",
        "########################################\n",
        "\n",
        "def create_q_network(state_dim, action_dim):\n",
        "    \"\"\"Create a Q-network that maps a single integer state to action Q-values.\"\"\"\n",
        "    # For a grid of size n x n, state_dim = n, but we interpret states in [0, n^2 - 1].\n",
        "    model = tf.keras.Sequential()\n",
        "    # Embedding to handle discrete states:\n",
        "    model.add(tf.keras.layers.Embedding(input_dim=env.observation_space.n, output_dim=64, input_length=1))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(env.action_space.n, activation='linear'))\n",
        "\n",
        "    # Build the model with a sample input\n",
        "    sample_input = tf.keras.Input(shape=(1,), dtype=tf.int32)\n",
        "    model(sample_input)  # \"call\" once to build\n",
        "    return model\n",
        "\n",
        "def create_prediction_network(state_dim):\n",
        "    \"\"\"Create a network to predict differences (N1 or N2).\"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=env.observation_space.n, output_dim=64, input_length=2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(256, activation=None),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.ReLU(),\n",
        "        tf.keras.layers.Dense(128, activation=None),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.ReLU(),\n",
        "        tf.keras.layers.Dense(64, activation=None),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.ReLU(),\n",
        "        tf.keras.layers.Dense(1)  # outputs a single scalar difference\n",
        "    ])\n",
        "\n",
        "    # Build with a sample input [batch, 2]\n",
        "    sample_input = tf.keras.Input(shape=(2,), dtype=tf.int32)\n",
        "    model(sample_input)\n",
        "    return model\n",
        "\n",
        "\n",
        "########################################\n",
        "# 3. The Reshuffle Agent\n",
        "########################################\n",
        "\n",
        "class ReshuffleAgent:\n",
        "    def __init__(self, state_dim, action_dim, config: Config):\n",
        "        self.config = config\n",
        "        self.step_count = 0\n",
        "\n",
        "        # tau_override is used if the KL exceeds a threshold\n",
        "        self.tau_override = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
        "\n",
        "        # Primary networks\n",
        "        self.Q_RS = create_q_network(state_dim, action_dim)\n",
        "        self.Q_D  = create_q_network(state_dim, action_dim)\n",
        "\n",
        "        # Prediction networks for differences\n",
        "        self.N1   = create_prediction_network(state_dim)  # Q_RS_k - BQ_RS_(k-1)\n",
        "        self.N2   = create_prediction_network(state_dim)  # Q_RS_k - Q_D_k\n",
        "\n",
        "        # Target networks\n",
        "        self.target_Q_RS = create_q_network(state_dim, action_dim)\n",
        "        self.target_Q_D  = create_q_network(state_dim, action_dim)\n",
        "\n",
        "        # \"Old\" networks for iteration k-1 references\n",
        "        self.Q_RS_old     = create_q_network(state_dim, action_dim)\n",
        "        self.Q_D_old      = create_q_network(state_dim, action_dim)\n",
        "        self.target_Q_RS_old = create_q_network(state_dim, action_dim)\n",
        "        self.target_Q_D_old  = create_q_network(state_dim, action_dim)\n",
        "\n",
        "        # Sync initial weights\n",
        "        self.sync_networks()\n",
        "        self.sync_old_networks()\n",
        "\n",
        "        # Create optimizers with a learning rate schedule\n",
        "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "            initial_learning_rate=config.initial_learning_rate,\n",
        "            decay_steps=10000,\n",
        "            decay_rate=0.99,\n",
        "            staircase=True\n",
        "        )\n",
        "        lr_schedule2 = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "            initial_learning_rate=config.initial_learning_rate,\n",
        "            decay_steps=10000,\n",
        "            decay_rate=0.99,\n",
        "            staircase=True\n",
        "        )\n",
        "        self.optimizer_rs = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "        self.optimizer_d  = tf.keras.optimizers.Adam(learning_rate=lr_schedule2)\n",
        "        self.optimizer_n1 = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "        self.optimizer_n2 = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    def sync_networks(self):\n",
        "        \"\"\"Hard update target networks from current networks.\"\"\"\n",
        "        self.target_Q_RS.set_weights(self.Q_RS.get_weights())\n",
        "        self.target_Q_D.set_weights(self.Q_D.get_weights())\n",
        "\n",
        "    def sync_old_networks(self):\n",
        "        \"\"\"Hard update the 'old' networks from the current networks.\"\"\"\n",
        "        self.Q_RS_old.set_weights(self.Q_RS.get_weights())\n",
        "        self.Q_D_old.set_weights(self.Q_D.get_weights())\n",
        "        self.target_Q_RS_old.set_weights(self.target_Q_RS.get_weights())\n",
        "        self.target_Q_D_old.set_weights(self.target_Q_D.get_weights())\n",
        "\n",
        "    def soft_update_networks(self):\n",
        "        \"\"\"Soft update target networks.\"\"\"\n",
        "        tau = self.config.soft_update_tau\n",
        "        for target, source in zip(self.target_Q_RS.weights, self.Q_RS.weights):\n",
        "            target.assign(tau * source + (1 - tau) * target)\n",
        "        for target, source in zip(self.target_Q_D.weights, self.Q_D.weights):\n",
        "            target.assign(tau * source + (1 - tau) * target)\n",
        "\n",
        "    @tf.function\n",
        "    def bellman_operator(self, rewards, next_states, dones, q_network, target_network):\n",
        "        \"\"\"Double Q-Learning Bellman operator.\"\"\"\n",
        "        # Step 1: Use online Q-network to SELECT the action\n",
        "        online_q_values = q_network(next_states)  # [batch, action_dim]\n",
        "        best_actions = tf.argmax(online_q_values, axis=1)  # [batch]\n",
        "\n",
        "        # Step 2: Use target network to EVALUATE that action\n",
        "        target_q_values = target_network(next_states)  # [batch, action_dim]\n",
        "        next_q_values = tf.reduce_sum(\n",
        "            target_q_values * tf.one_hot(best_actions, target_q_values.shape[-1]),\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        return rewards + self.config.gamma * next_q_values * (1 - dones)\n",
        "    @tf.function\n",
        "    def calculate_weights(self, states, next_states, actions,\n",
        "                          q_rs_values, q_d_values, p_d_batch, q_diff_pred):\n",
        "        \"\"\"\n",
        "        Returns the 'weights' for the Q_RS loss, reshuffled from p^D(s,a).\n",
        "        p_d_batch: dataset distribution p^D(s,a) for each sample.\n",
        "        q_diff_pred: predicted difference from N1 (or whichever you need).\n",
        "        \"\"\"\n",
        "\n",
        "        # 1) Compute the log score (log_p_rs). For example:\n",
        "        #    (Below is just an example \"score\" combining N1, N2, etc.)\n",
        "        n1_pred = self.N1(tf.stack([states, next_states], axis=1))[:, 0]\n",
        "        n2_pred = self.N2(tf.stack([states, next_states], axis=1))[:, 0]\n",
        "\n",
        "        # Decide which tau to use (same as your existing logic)\n",
        "        tau = tf.cond(\n",
        "            self.tau_override > 0.0,\n",
        "            lambda: self.tau_override,\n",
        "            lambda: tf.cast(self.config.get_tau(self.step_count), tf.float32)\n",
        "        )\n",
        "\n",
        "        # Example of a log-score:\n",
        "        #   log_p_rs = [   -(1 + |n2_pred|)/tau   +  log(|n1_pred|+1e-8)   ]\n",
        "        #   (You can customize to your paper’s approach.)\n",
        "        log_p_rs = tf.exp(- (1.0 +((n2_pred))) / tau )\\\n",
        "                  *tf.abs(n1_pred)\n",
        "        q =(log_p_rs)  # shape [batch]\n",
        "\n",
        "        # 3) Build the reshuffled distribution p_rs ~ p_d * q\n",
        "        numerator = q\n",
        "        denom = tf.reduce_mean(numerator) + 1e-8\n",
        "        log_p_rs = numerator / denom  # This is your new distribution p_rs(s,a)\n",
        "\n",
        "        # 2) Convert to unnormalized \"preference\" q via softmax\n",
        "        #    NOTE: We typically do log_p_rs / tau again if needed,\n",
        "        #    but you already included a /tau above. Adjust if desired.\n",
        "        q = tf.nn.softmax(log_p_rs)  # shape [batch]\n",
        "\n",
        "        # 3) Build the reshuffled distribution p_rs ~ p_d * q\n",
        "        numerator = q *p_d_batch\n",
        "        denom = tf.reduce_sum(numerator) + 1e-8\n",
        "        p_rs = numerator / denom  # This is your new distribution p_rs(s,a)\n",
        "\n",
        "        ratio_standard = p_rs / (p_d_batch + 1e-8)\n",
        "\n",
        "        # 1) Compute 99th percentile of ratio\n",
        "        #    (Requires e.g. 'tfp' or your own percentile code)\n",
        "        percentile_q = 99.0\n",
        "        # If you have tfp:\n",
        "        # import tensorflow_probability as tfp\n",
        "        # ratio_percentile_val = tfp.stats.percentile(ratio_standard, q=percentile_q)\n",
        "        # Or roll your own percentile with tf.sort(...)\n",
        "\n",
        "        # Quick approximate percentile:\n",
        "        ratio_sorted = tf.sort(ratio_standard)\n",
        "        idx_99 = tf.cast(tf.math.floor(0.99 * tf.cast(tf.size(ratio_sorted), tf.float32)), tf.int32)\n",
        "\n",
        "        ratio_percentile_val = ratio_sorted[idx_99]\n",
        "\n",
        "        # 2) Clip\n",
        "        ratio_clipped = tf.minimum(ratio_standard, ratio_percentile_val)\n",
        "        weights = ratio_clipped\n",
        "\n",
        "        # Optional: track KL\n",
        "        kl_div = tf.reduce_sum(\n",
        "            p_rs * (  # Use p_rs instead of ratio_standard\n",
        "                tf.math.log(p_rs + 1e-8)\n",
        "                - tf.math.log(p_d_batch + 1e-8)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return log_p_rs, kl_div, tau\n",
        "\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, states, actions, rewards, next_states, dones, p_d):\n",
        "        \"\"\"One training step, referencing old Q-networks for difference computations.\"\"\"\n",
        "        self.step_count += 1\n",
        "\n",
        "        # 1. Current Q-values\n",
        "        q_rs = self.Q_RS(states)  # [batch, action_dim]\n",
        "        q_d  = self.Q_D(states)   # [batch, action_dim]\n",
        "\n",
        "        # For chosen actions\n",
        "        q_rs_values = tf.reduce_sum(q_rs * tf.one_hot(actions, q_rs.shape[-1]), axis=1)\n",
        "        q_d_values  = tf.reduce_sum(q_d  * tf.one_hot(actions, q_d.shape[-1]), axis=1)\n",
        "\n",
        "        # 2. Bellman backups using OLD Q_RS\n",
        "        bq_rs_old = self.bellman_operator(rewards, next_states, dones,\n",
        "                                          self.Q_RS_old, self.target_Q_RS_old)\n",
        "\n",
        "        # 3. Differences from N1, N2\n",
        "        s_ns_stack = tf.stack([tf.squeeze(states, axis=1),\n",
        "                               tf.squeeze(next_states, axis=1)], axis=1)\n",
        "        n1_pred = self.N1(s_ns_stack)[:, 0]\n",
        "        n2_pred = self.N2(s_ns_stack)[:, 0]\n",
        "\n",
        "        # 4. p^D(s,a) for the batch\n",
        "        states_squeezed = tf.squeeze(states, axis=1)\n",
        "        batch_indices = states_squeezed * q_rs.shape[-1] + actions\n",
        "        p_d_batch = tf.gather(p_d, batch_indices)\n",
        "\n",
        "        # 5. Calculate final weights for Q_RS update\n",
        "        weights, kl_div, actual_tau_used = self.calculate_weights(\n",
        "            states_squeezed, tf.squeeze(next_states, axis=1),\n",
        "            actions, q_rs_values, q_d_values, p_d_batch, n1_pred\n",
        "        )\n",
        "\n",
        "        # Adjust tau if KL is too large\n",
        "        kl_div_threshold = self.config.kl_div_threshold\n",
        "        new_tau = actual_tau_used\n",
        "        self.tau_override.assign(new_tau)\n",
        "\n",
        "        # 6. Compute Bellman backup for the CURRENT Q_RS\n",
        "        bq_rs_current = self.bellman_operator(rewards, next_states, dones,\n",
        "                                              self.Q_RS, self.target_Q_RS)\n",
        "\n",
        "        # 7. Update Q_RS (weighted MSE + conservative penalty)\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_rs_actions_current = tf.reduce_sum(self.Q_RS(states) *\n",
        "                                                 tf.one_hot(actions, q_rs.shape[-1]), axis=1)\n",
        "            # Weighted MSE\n",
        "            bellman_loss = tf.reduce_mean(weights * tf.square(bq_rs_current - q_rs_actions_current))\n",
        "            # Conservative penalty\n",
        "            conservative_penalty = self.config.lambda_cql * tf.reduce_mean(tf.reduce_max(q_rs, axis=1))\n",
        "            loss_rs = bellman_loss + conservative_penalty\n",
        "\n",
        "        grads_rs = tape.gradient(loss_rs, self.Q_RS.trainable_variables)\n",
        "        self.optimizer_rs.apply_gradients(zip(grads_rs, self.Q_RS.trainable_variables))\n",
        "\n",
        "        # 8. Update Q_D (standard MSE)\n",
        "        bq_d_current = self.bellman_operator(rewards, next_states, dones,\n",
        "                                             self.Q_D, self.target_Q_D)\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_d_actions_current = tf.reduce_sum(self.Q_D(states) *\n",
        "                                                tf.one_hot(actions, q_d.shape[-1]), axis=1)\n",
        "            loss_d = tf.reduce_mean(tf.square(bq_d_current - q_d_actions_current))\n",
        "        grads_d = tape.gradient(loss_d, self.Q_D.trainable_variables)\n",
        "        self.optimizer_d.apply_gradients(zip(grads_d, self.Q_D.trainable_variables))\n",
        "\n",
        "        # 9. Update N1 => (Q_k^{RS}(s,a) - BQ_{k-1}^{RS}(s,a))\n",
        "        current_q_rs_actions = tf.reduce_sum(self.Q_RS(states) *\n",
        "                                             tf.one_hot(actions, q_rs.shape[-1]), axis=1)\n",
        "        with tf.GradientTape() as tape:\n",
        "            n1_target = current_q_rs_actions - bq_rs_old\n",
        "            n1_values = self.N1(s_ns_stack)[:, 0]\n",
        "            loss_n1 = tf.reduce_mean(tf.square(n1_target - n1_values))\n",
        "\n",
        "        grads_n1 = tape.gradient(loss_n1, self.N1.trainable_variables)\n",
        "        self.optimizer_n1.apply_gradients(zip(grads_n1, self.N1.trainable_variables))\n",
        "\n",
        "        # 10. Update N2 => (Q_k^{RS}(s,a) - Q_k^D(s,a))\n",
        "        current_q_d_actions = tf.reduce_sum(self.Q_D(states) *\n",
        "                                            tf.one_hot(actions, q_d.shape[-1]), axis=1)\n",
        "        with tf.GradientTape() as tape:\n",
        "            n2_target = current_q_rs_actions - current_q_d_actions\n",
        "            n2_values = self.N2(s_ns_stack)[:, 0]\n",
        "            loss_n2 = tf.reduce_mean(tf.square(n2_target - n2_values))\n",
        "\n",
        "        grads_n2 = tape.gradient(loss_n2, self.N2.trainable_variables)\n",
        "        self.optimizer_n2.apply_gradients(zip(grads_n2, self.N2.trainable_variables))\n",
        "\n",
        "        # 11. Soft update targets if needed\n",
        "        if (self.step_count % self.config.target_update_freq) == 0:\n",
        "            self.soft_update_networks()\n",
        "\n",
        "        # -----------------------------------------------------\n",
        "        # Extra metrics for debugging / plotting:\n",
        "        #   - Bellman error for Q_RS\n",
        "        #   - Value error between Q_RS and Q_D\n",
        "        #   - Average weight for this batch\n",
        "        # -----------------------------------------------------\n",
        "        bellman_error_batch = tf.reduce_mean(tf.square(bq_rs_current - q_rs_actions_current))\n",
        "        value_error_batch   = tf.reduce_mean(tf.square(q_rs_values - q_d_values))\n",
        "        weight_mean_batch   = tf.reduce_mean(weights)\n",
        "\n",
        "        return (loss_rs, loss_d, loss_n1, loss_n2, kl_div,\n",
        "                bellman_error_batch, value_error_batch, weight_mean_batch)\n",
        "\n",
        "\n",
        "    def estimate_p_d(self, states, actions, num_states, num_actions, alpha=0.01):\n",
        "        \"\"\"\n",
        "        Estimates behavior policy distribution p_d(s,a) using empirical frequency.\n",
        "\n",
        "        Args:\n",
        "            states: Array of states from trajectories\n",
        "            actions: Array of actions from trajectories\n",
        "            num_states: Total number of possible states\n",
        "            num_actions: Total number of possible actions\n",
        "            alpha: Smoothing parameter (default: 0.01)\n",
        "        \"\"\"\n",
        "        # Input validation\n",
        "        if len(states) != len(actions):\n",
        "            raise ValueError(\"States and actions must have same length\")\n",
        "        if alpha < 0:\n",
        "            raise ValueError(\"Alpha must be non-negative\")\n",
        "\n",
        "        total_space = num_states * num_actions\n",
        "        counts = np.zeros(total_space)\n",
        "\n",
        "        # Get trajectory-aware counts\n",
        "        indices = states * num_actions + actions\n",
        "        unique_indices, visit_counts = np.unique(indices, return_counts=True)\n",
        "        counts[unique_indices] = visit_counts\n",
        "\n",
        "        # Compute total visits per state for conditional probability\n",
        "        state_visits = np.zeros(num_states)\n",
        "        for s in range(num_states):\n",
        "            state_visits[s] = np.sum(counts[s * num_actions:(s + 1) * num_actions])\n",
        "\n",
        "        # Add smoothing weighted by state visitation frequency\n",
        "        state_freq = state_visits / (np.sum(state_visits) + 1e-8)\n",
        "        for s in range(num_states):\n",
        "            start_idx = s * num_actions\n",
        "            end_idx = (s + 1) * num_actions\n",
        "            # More smoothing for rarely visited states\n",
        "            local_alpha = alpha * (1 + (1 - state_freq[s]))\n",
        "            counts[start_idx:end_idx] += local_alpha\n",
        "\n",
        "        # Normalize to get probability distribution\n",
        "        p_d = counts / np.sum(counts)\n",
        "\n",
        "        # Verify distribution properties\n",
        "        assert np.all(p_d >= 0), \"All probabilities must be non-negative\"\n",
        "        assert np.abs(np.sum(p_d) - 1.0) < 1e-6, \"Probabilities must sum to 1\"\n",
        "\n",
        "        return p_d\n",
        "\n",
        "########################################\n",
        "# 4. Training Loop\n",
        "########################################\n",
        "\n",
        "def train_offline_rl(agent: ReshuffleAgent, dataset, num_epochs=250):\n",
        "    \"\"\"\n",
        "    Trains the agent from a static offline dataset.\n",
        "    dataset is a list of tuples: (state, action, reward, next_state, done).\n",
        "    \"\"\"\n",
        "    dataset=deepcopy(dataset)\n",
        "    # 1. Estimate p_d from the entire dataset (once)\n",
        "    all_states  = np.array([s for (s, _, _, _, _) in dataset]).flatten()\n",
        "    all_actions = np.array([a for (_, a, _, _, _) in dataset])\n",
        "    p_d = agent.estimate_p_d(\n",
        "        all_states, all_actions,\n",
        "        agent.config.n**2,\n",
        "        agent.Q_RS.output_shape[-1]\n",
        "    )\n",
        "    p_d = tf.constant(p_d, dtype=tf.float32)\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Global lists to store step-level metrics.\n",
        "    # We'll append to these every mini-batch (step).\n",
        "    # -------------------------------------------------\n",
        "    bellman_errors_by_step = []\n",
        "    value_errors_by_step   = []\n",
        "    weights_by_step        = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # 2. Sync \"old\" networks at start of each epoch\n",
        "        agent.sync_old_networks()\n",
        "        if epoch%10==0:\n",
        "          reward_p.append(test_learned_policy(num_test_episodes=4,model=agent.Q_RS,print_e=False))\n",
        "\n",
        "        total_loss_rs = 0.0\n",
        "        total_loss_d  = 0.0\n",
        "        total_loss_n1 = 0.0\n",
        "        total_loss_n2 = 0.0\n",
        "        total_kl_div  = 0.0\n",
        "        epoch_rewards = 0.0\n",
        "        count_batches = 0\n",
        "\n",
        "        # Shuffle dataset if desired\n",
        "        random.shuffle(dataset)\n",
        "\n",
        "        # 3. Process data in mini-batches\n",
        "        for i in range(0, len(dataset), agent.config.batch_size):\n",
        "            batch = dataset[i:i + agent.config.batch_size]\n",
        "            states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "            # Convert to tensors\n",
        "            states_tf      = tf.convert_to_tensor(np.array(states).reshape(-1, 1), dtype=tf.int32)\n",
        "            next_states_tf = tf.convert_to_tensor(np.array(next_states).reshape(-1, 1), dtype=tf.int32)\n",
        "            actions_tf     = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
        "            rewards_tf     = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "            dones_tf       = tf.convert_to_tensor(dones,   dtype=tf.float32)\n",
        "\n",
        "            (loss_rs, loss_d, loss_n1, loss_n2, kl_div,\n",
        "             bellman_err_b, value_err_b, weight_mean_b) = agent.train_step(\n",
        "                states_tf, actions_tf, rewards_tf,\n",
        "                next_states_tf, dones_tf, p_d\n",
        "            )\n",
        "\n",
        "            # Accumulate epoch-level sums\n",
        "            total_loss_rs += loss_rs\n",
        "            total_loss_d  += loss_d\n",
        "            total_loss_n1 += loss_n1\n",
        "            total_loss_n2 += loss_n2\n",
        "            total_kl_div  += kl_div\n",
        "            epoch_rewards += tf.reduce_sum(rewards_tf)\n",
        "            count_batches += 1\n",
        "\n",
        "            # ----------------------------------\n",
        "            # Store these step-level metrics NOW\n",
        "            # ----------------------------------\n",
        "            bellman_errors_by_step.append(bellman_err_b.numpy())\n",
        "            value_errors_by_step.append(value_err_b.numpy())\n",
        "            weights_by_step.append(weight_mean_b.numpy())\n",
        "\n",
        "        if count_batches > 0:\n",
        "            avg_loss_rs = total_loss_rs / count_batches\n",
        "            avg_loss_d  = total_loss_d / count_batches\n",
        "            avg_loss_n1 = total_loss_n1 / count_batches\n",
        "            avg_loss_n2 = total_loss_n2 / count_batches\n",
        "            avg_kl      = total_kl_div / count_batches\n",
        "        else:\n",
        "            # No batches processed at all (rare, if dataset is non-empty)\n",
        "            avg_loss_rs = 0\n",
        "            avg_loss_d  = 0\n",
        "            avg_loss_n1 = 0\n",
        "            avg_loss_n2 = 0\n",
        "            avg_kl      = 0\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "              f\"Loss Q_RS: {avg_loss_rs:.6f}, \"\n",
        "              f\"Loss Q_D: {avg_loss_d:.6f}, \"\n",
        "              f\"Loss N1: {avg_loss_n1:.6f}, \"\n",
        "              f\"Loss N2: {avg_loss_n2:.6f}, \"\n",
        "              f\"KL: {avg_kl:.6f}, \"\n",
        "              f\"Total R: {epoch_rewards.numpy():.2f}\")\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # After training, we can optionally plot or save the step-level data\n",
        "    # ------------------------------------------------------------------\n",
        "    plt.figure(figsize=(16,4))\n",
        "\n",
        "    # 1) Bellman Error by step\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.plot(bellman_errors_by_step, label=\"Bellman Error per Step\")\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"MSE\")\n",
        "    plt.title(\"Bellman Error (per mini-batch)\")\n",
        "    plt.legend()\n",
        "\n",
        "    # 2) Value Error by step\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.plot(value_errors_by_step, color='green', label=\"Value Error per Step\")\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"MSE (Q_RS - Q_D)\")\n",
        "    plt.title(\"Value Error (per mini-batch)\")\n",
        "    plt.legend()\n",
        "\n",
        "    # 3) Weights by step\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.plot(weights_by_step, color='red', label=\"Mean Weights per Step\")\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"Mean( p_RS / p_D )\")\n",
        "    plt.title(\"Reshuffling Weights (per mini-batch)\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # If desired, return step-level lists so you can save them externally\n",
        "    return bellman_errors_by_step, value_errors_by_step, weights_by_step\n",
        "\n",
        "\n",
        "########################################\n",
        "# 5. Evaluation (Optional)\n",
        "########################################\n",
        "\n",
        "def evaluate_agent(env, agent: ReshuffleAgent, num_episodes=50):\n",
        "    \"\"\"\n",
        "    Evaluates the agent by acting greedily w.r.t. Q_RS.\n",
        "    Returns average reward over num_episodes.\n",
        "    \"\"\"\n",
        "    total_rewards = 0\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            state_tensor = tf.convert_to_tensor([[state]], dtype=tf.int32)\n",
        "            q_values = agent.Q_RS(state_tensor)  # shape [1, action_dim]\n",
        "            action = tf.argmax(q_values[0]).numpy()\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "        total_rewards += episode_reward\n",
        "    return total_rewards / num_episodes\n",
        "\n",
        "\n",
        "########################################\n",
        "# 6. Usage Example\n",
        "########################################\n",
        "if __name__ == \"__main__\":\n",
        "    config = Config()\n",
        "    set_seeds()\n",
        "\n",
        "    # Make sure dataset is provided\n",
        "    if 'dataset' not in locals():\n",
        "        raise ValueError(\"Dataset not defined. Please provide training dataset.\")\n",
        "\n",
        "    # Validate environment\n",
        "    if 'env' not in locals():\n",
        "        raise ValueError(\"Environment not defined. Please provide environment.\")\n",
        "\n",
        "    # Create agent\n",
        "    agent = ReshuffleAgent(state_dim=env.observation_space.n, action_dim=env.action_space.n , config=config)\n",
        "\n",
        "    # Compile models\n",
        "    agent.Q_RS.compile(optimizer=agent.optimizer_rs, loss='mse')\n",
        "    agent.Q_D.compile(optimizer=agent.optimizer_d, loss='mse')\n",
        "    agent.N1.compile(optimizer=agent.optimizer_n1, loss='mse')\n",
        "    agent.N2.compile(optimizer=agent.optimizer_n2, loss='mse')\n",
        "\n",
        "    # Train agent\n",
        "    with tf.device('/GPU:0'):\n",
        "        bellman_steps, value_steps, weight_steps = train_offline_rl(agent, dataset, num_epochs=200)\n",
        "\n",
        "    # Save model\n",
        "    model_save_dir = \"saved_models\"\n",
        "    os.makedirs(model_save_dir, exist_ok=True)\n",
        "    agent.Q_RS.save(os.path.join(model_save_dir, \"taxi_model_maxsteps_66.keras\"))\n",
        "\n",
        "    # Optionally evaluate\n",
        "    # avg_reward = evaluate_agent(env, agent)\n",
        "    # print(\"Average Evaluation Reward:\", avg_reward)"
      ]
    }
  ]
}